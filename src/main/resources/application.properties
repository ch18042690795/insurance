#端口
server.port=8099
##### druid ####
# JDBC 配置(驱动类自动从url的mysql识别,数据源类型自动识别)
spring.datasource.druid.url=jdbc:mysql://localhost:3306/insurance?serverTimezone=UTC&useUnicode=true&characterEncoding=UTF8
spring.datasource.druid.username=root
spring.datasource.druid.password=root
spring.datasource.druid.driver-class-name=com.mysql.jdbc.Driver
#连接池配置
spring.datasource.druid.initial-size=1
spring.datasource.druid.max-active=20
spring.datasource.druid.min-idle=1
spring.datasource.druid.max-wait=60000
spring.datasource.druid.filters=stat
##### druid 监控 ####
# WebStatFilter配置
spring.datasource.druid.web-stat-filter.enabled=true
spring.datasource.druid.web-stat-filter.url-pattern=/*
spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*
spring.datasource.druid.web-stat-filter.session-stat-enable=false
spring.datasource.druid.web-stat-filter.session-stat-max-count=1000
spring.datasource.druid.web-stat-filter.principal-session-name=admin
spring.datasource.druid.web-stat-filter.principal-cookie-name=admin
spring.datasource.druid.web-stat-filter.profile-enable=true
# StatViewServlet配置
spring.datasource.druid.stat-view-servlet.enabled=true
spring.datasource.druid.stat-view-servlet.url-pattern=/druid/*
spring.datasource.druid.stat-view-servlet.reset-enable=true
spring.datasource.druid.stat-view-servlet.login-username=admin
spring.datasource.druid.stat-view-servlet.login-password=admin
#mybatis
mybatis.mapper-locations=classpath:mapping/*.xml
##### 日志 #####
logging.level.root=info
# 打印sql日志
logging.level.com.middle.dao=debug
logging.file=insurance.log
#表格文件下载位置
file.location=F:/excels
#============== kafka ===================
spring.kafka.producer.retries=0
spring.kafka.producer.batch.size=16384
spring.kafka.producer.linger=1
spring.kafka.producer.buffer.memory=40960
spring.application.name=kafka-producer
#kafka configuration
spring.kafka.producer.bootstrap-servers=127.0.0.1:9099
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#topic
#三大目录主题
spring.kafka.app.topic.foo=test
#医疗机构接口主题
spring.kafka.app.topic.foo1=test1
#参保人信息主题
spring.kafka.app.topic.foo2=test2
#大病索赔主题
spring.kafka.app.topic.foo3=test3
#住院补偿主题
spring.kafka.app.topic.foo4=test4
#大病补偿业务状态主题
spring.kafka.app.topic.foo5=test5
#参保信息获取接口主题
spring.kafka.app.topic.foo6=test6
#理赔状态接口主题
spring.kafka.app.topic.foo7=test7
#大病补偿业务状态查询主题
spring.kafka.app.topic.foo8=test8
spring.kafka.app.topic.foo9=test9
spring.application.consumername=kafka-consumer
#kafka configuration
#指定消息被消费之后自动提交偏移量，以便下次继续消费
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100000
##指定消息组
#spring.kafka.consumer.group-id=guan
#指定kafka服务器地址
spring.kafka.consumer.bootstrap-servers=127.0.0.1:9099
#指定从最近地方开始消费(earliest)
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.listener.concurrency=5

kafka.topic.nums=9